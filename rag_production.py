import os
import time
import json
from dotenv import load_dotenv
from supabase import create_client, Client
from sentence_transformers import SentenceTransformer, CrossEncoder
from openai import OpenAI
from cachetools import TTLCache

# 1. –ó–ê–ì–†–£–ó–ö–ê –ù–ê–°–¢–†–û–ï–ö
load_dotenv()
supabase: Client = create_client(os.environ.get("SUPABASE_URL"), os.environ.get("SUPABASE_KEY"))
openai_client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# –ß–∏—Ç–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –∏–∑ .env
RERANK_ENABLED = os.environ.get("RERANK_ENABLED", "false").lower() == "true"
USE_CACHE = os.environ.get("USE_CACHE", "false").lower() == "true"
TOP_K = int(os.environ.get("RETRIEVAL_K", 10)) # –°–∫–æ–ª—å–∫–æ –∏—Å–∫–∞—Ç—å
TOP_N = int(os.environ.get("RERANK_N", 3))     # –°–∫–æ–ª—å–∫–æ –æ—Å—Ç–∞–≤–ª—è—Ç—å

# 2. –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ú–û–î–ï–õ–ï–ô
print("‚è≥ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª–∏ (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)...")
# –ú–æ–¥–µ–ª—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ (Bi-Encoder)
embed_model = SentenceTransformer("all-MiniLM-L6-v2")
# –ú–æ–¥–µ–ª—å –¥–ª—è —Ç–æ—á–Ω–æ–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ (Cross-Encoder) - –æ–Ω–∞ —É–º–Ω–µ–µ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ
rerank_model = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

# 3. –ù–ê–°–¢–†–û–ô–ö–ê –ö–ï–®–ê (–•—Ä–∞–Ω–∏–º –¥–æ 100 –æ—Ç–≤–µ—Ç–æ–≤, –∂–∏–≤—É—Ç CACHE_TTL —Å–µ–∫—É–Ω–¥)
cache = TTLCache(maxsize=100, ttl=int(os.environ.get("CACHE_TTL", 60)))

# --- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ---

def search_vectors(query):
    vector = embed_model.encode(query).tolist()
    response = supabase.rpc("match_documents", {
        "query_embedding": vector,
        "match_threshold": 0.1, # –ü–æ—Ä–æ–≥ –Ω–∏–∂–µ, —á—Ç–æ–±—ã –Ω–∞–±—Ä–∞—Ç—å –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
        "match_count": TOP_K
    }).execute()
    return response.data if response.data else []

def search_keywords(query):
    response = supabase.rpc("kw_match_documents", {
        "query_text": query,
        "match_count": TOP_K
    }).execute()
    return response.data if response.data else []

# –§—É–Ω–∫—Ü–∏—è RRF –∏–∑ –ø—Ä–æ—à–ª–æ–≥–æ —É—Ä–æ–∫–∞
def rrf_fusion(semantic_results, keyword_results, k=60):
    fused_scores = {}
    doc_content = {} 
    
    for doc_list in [semantic_results, keyword_results]:
        for rank, doc in enumerate(doc_list):
            doc_id = doc['id']
            doc_content[doc_id] = doc 
            if doc_id not in fused_scores: fused_scores[doc_id] = 0
            fused_scores[doc_id] += 1 / (rank + k)
            
    sorted_ids = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)
    return [doc_content[doc_id] for doc_id, score in sorted_ids] # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å —Å–ø–∏—Å–æ–∫

# --- –ì–õ–ê–í–ù–ê–Ø –õ–û–ì–ò–ö–ê (PIPELINE) ---

def ask_smart_bot(question):
    start_time = time.time() # –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è
    print(f"\nüë§ –í–æ–ø—Ä–æ—Å: {question}")

    # 1. –ü–†–û–í–ï–†–ö–ê –ö–ï–®–ê
    if USE_CACHE and question in cache:
        print(f"‚ö° CACHE HIT! –û—Ç–≤–µ—Ç –Ω–∞–π–¥–µ–Ω –≤ –ø–∞–º—è—Ç–∏.")
        print("="*50)
        print(f"ü§ñ –û–¢–í–ï–¢:\n{cache[question]}")
        print("="*50)
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {time.time() - start_time:.4f} —Å–µ–∫ (–ú–≥–Ω–æ–≤–µ–Ω–Ω–æ!)")
        return

    # 2. –ü–û–ò–°–ö (RETRIEVAL)
    t1 = time.time()
    vec_res = search_vectors(question)
    kw_res = search_keywords(question)
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–µ—Ä–µ–∑ RRF
    candidates = rrf_fusion(vec_res, kw_res)
    print(f"üîç –ù–∞–π–¥–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(candidates)} (–∑–∞ {time.time() - t1:.4f} —Å–µ–∫)")

    # 3. –ü–ï–†–ï–†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–ï (RERANKING)
    final_docs = candidates
    if RERANK_ENABLED and candidates:
        t2 = time.time()
        print("‚öñÔ∏è  –ó–∞–ø—É—Å–∫–∞—é Re-ranking (Cross-Encoder)...")
        
        # –ì–æ—Ç–æ–≤–∏–º –ø–∞—Ä—ã [–í–æ–ø—Ä–æ—Å, –¢–µ–∫—Å—Ç] –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
        pairs = [[question, doc['content']] for doc in candidates]
        
        # –ù–µ–π—Ä–æ—Å–µ—Ç—å –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å
        scores = rerank_model.predict(pairs)
        
        # –ü—Ä–∏–∫—Ä–µ–ø–ª—è–µ–º –æ—Ü–µ–Ω–∫–∏ –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
        ranked_docs = []
        for i, doc in enumerate(candidates):
            ranked_docs.append({'doc': doc, 'score': scores[i]})
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ—Ü–µ–Ω–∫–µ (–æ—Ç –≤—ã—Å–æ–∫–æ–π –∫ –Ω–∏–∑–∫–æ–π)
        ranked_docs = sorted(ranked_docs, key=lambda x: x['score'], reverse=True)
        
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –¢–û–ü-N –ª—É—á—à–∏—Ö
        final_docs = [item['doc'] for item in ranked_docs[:TOP_N]]
        
        print(f"‚úÖ Re-ranking –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {time.time() - t2:.4f} —Å–µ–∫.")
        print(f"   –õ—É—á—à–∏–π –¥–æ–∫—É–º–µ–Ω—Ç (Score: {ranked_docs[0]['score']:.4f}): {final_docs[0]['content'][:50]}...")
    else:
        final_docs = candidates[:TOP_N] # –ü—Ä–æ—Å—Ç–æ –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –ø–æ–ø–∞–≤—à–∏–µ—Å—è

    # 4. –ì–ï–ù–ï–†–ê–¶–ò–Ø (GENERATION)
    t3 = time.time()
    print("üß† –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ GPT...")
    
    context_text = "\n---\n".join([d['content'] for d in final_docs])
    
    response = openai_client.chat.completions.create(
        messages=[
            {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç. –û—Ç–≤–µ—á–∞–π –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç."},
            {"role": "user", "content": f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context_text}\n\n–í–æ–ø—Ä–æ—Å: {question}"}
        ],
        model="gpt-3.5-turbo",
    )
    answer = response.choices[0].message.content
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
    if USE_CACHE:
        cache[question] = answer

    total_time = time.time() - start_time
    print("\n" + "="*50)
    print(f"ü§ñ –û–¢–í–ï–¢:\n{answer}")
    print("="*50)
    print(f"‚è±Ô∏è –ü–æ–ª–Ω–æ–µ –≤—Ä–µ–º—è: {total_time:.4f} —Å–µ–∫")
    print(f"üìä –ú–µ—Ç—Ä–∏–∫–∏: –ü–æ–∏—Å–∫={t3-start_time:.2f}s | GPT={total_time-(t3-start_time):.2f}s")

if __name__ == "__main__":
    while True:
        q = input("\n–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å (–∏–ª–∏ 'exit'): ")
        if q.lower() == 'exit': break
        ask_smart_bot(q)