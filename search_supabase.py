import os
from dotenv import load_dotenv
from supabase import create_client, Client
from sentence_transformers import SentenceTransformer

# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫
load_dotenv()
url = os.environ.get("SUPABASE_URL")
key = os.environ.get("SUPABASE_KEY")
supabase: Client = create_client(url, key)

# 2. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç—É –∂–µ –º–æ–¥–µ–ª—å (—á—Ç–æ–±—ã "—è–∑—ã–∫" –∑–∞–ø—Ä–æ—Å–∞ —Å–æ–≤–ø–∞–¥–∞–ª —Å –±–∞–∑–æ–π)
model = SentenceTransformer("all-MiniLM-L6-v2")

def search(query):
    print(f"\nüîé –í–æ–ø—Ä–æ—Å: '{query}'")
    
    # –ê. –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤–æ–ø—Ä–æ—Å –≤ –≤–µ–∫—Ç–æ—Ä
    query_vector = model.encode(query).tolist()
    
    # –ë. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä –≤ Supabase (–≤—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é match_documents)
    response = supabase.rpc("match_documents", {
        "query_embedding": query_vector,
        "match_threshold": 0.3, # –ò—Å–∫–∞—Ç—å –¥–∞–∂–µ –æ—Ç–¥–∞–ª–µ–Ω–Ω–æ –ø–æ—Ö–æ–∂–∏–µ (0.3 - –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥)
        "match_count": 3        # –í–µ—Ä–Ω—É—Ç—å —Ç–æ–ø-3 –æ—Ç–≤–µ—Ç–∞
    }).execute()
    
    # –í. –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    if not response.data:
        print("‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ (–ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å).")
        return

    print("‚úÖ –ù–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ:")
    for i, doc in enumerate(response.data):
        print(f"--- –†–µ–∑—É–ª—å—Ç–∞—Ç #{i+1} (–°—Ö–æ–¥—Å—Ç–≤–æ: {doc['similarity']:.2f}) ---")
        print(f"üìÑ –¢–µ–∫—Å—Ç: {doc['content'][:200]}...") # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 200 –±—É–∫–≤
        print("-" * 50)

if __name__ == "__main__":
    # –¢–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∫ –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–∏
    search("–ö—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º –≤–ª–∞—Å—Ç–∏?")
    search("–ü—Ä–∞–≤–æ –Ω–∞ –æ—Ö—Ä–∞–Ω—É –∑–¥–æ—Ä–æ–≤—å—è")